{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_episodes = \"../exploration/BIG_DF_ML.ipynb\"\n",
    "\n",
    "db01 = pd.read_csv(\"../gitignore/title_basics_traite.csv\")\n",
    "db02 = pd.read_csv(\"../gitignore/title_ratings_final.tsv\", sep=\"\\t\")\n",
    "db03 = pd.read_csv(\"../gitignore/title.akas_final.tsv\", sep=\"\\t\")\n",
    "db04 = pd.read_csv(\"../gitignore/tmdb_ml_final.csv\")\n",
    "db05 = pd.read_csv(\"../gitignore/data_bechdel.csv\")\n",
    "db07 = pd.read_csv(\"../gitignore/name.basics.tsv\", sep=\"\\t\") \n",
    "db08 = pd.read_csv(\"../gitignore/title.crew.tsv\", sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db07[db07['nconst'] == 'nm0000318']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db08[db08['tconst'] == 'tt0107688']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbmerge_1 = pd.merge(db01, db02, right_on='title_ratings_tconst', left_on='tconst', how='left') #Title Basics + Title Ratings\n",
    "dbmerge_2 = pd.merge(dbmerge_1, db03, left_on='tconst', right_on='titleId', how='left') # + Title Akas\n",
    "dbmerge_3 = pd.merge(dbmerge_2, db04, left_on='tconst', right_on='tmdb_imdb_id', how='left') # + TMDB Full\n",
    "dbmerge_4 = pd.merge(dbmerge_3, db05, left_on='tconst', right_on='imdbid', how='left') # + Bechdel\n",
    "dbmerge_4 = pd.merge(dbmerge_4, db08, left_on='tconst', right_on='tconst', how='left') # + Title Crew\n",
    "dbmerge_4 = pd.merge(dbmerge_4, db07, left_on='directors', right_on='nconst', how='left') # + Name Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbmerge_4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbmerge_4.to_csv(\"../gitignore/BIG_DF_ML.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbmerge_4['movie'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML = dbmerge_4.drop(columns=[\n",
    "    'titleType',\n",
    "    'genres', \n",
    "    'decade', \n",
    "    'Adult',\n",
    "    'Short',\n",
    "    'movie',\n",
    "    'tmdb_Comedy',\n",
    "    'tmdb_Adventure',\n",
    "    'tmdb_Drama',\n",
    "    'tmdb_Crime',\n",
    "    'tmdb_Action',\n",
    "    'tmdb_Documentary',\n",
    "    'tmdb_Animation',\n",
    "    'tmdb_Mystery',\n",
    "    'tmdb_Horror',\n",
    "    'tmdb_Western',\n",
    "    'tmdb_Science Fiction',\n",
    "    'tmdb_Thriller',\n",
    "    'tmdb_Romance',\n",
    "    'tmdb_Fantasy',\n",
    "    'tmdb_Family',\n",
    "    'tmdb_History',\n",
    "    'tmdb_Music',\n",
    "    'tmdb_War', \n",
    "    'ordering',\n",
    "    'region',\n",
    "    'language',\n",
    "    'types',\n",
    "    'attributes',\n",
    "    'isOriginalTitle',\n",
    "    'birthYear',\n",
    "    'deathYear',\n",
    "    'primaryProfession',\n",
    "    'knownForTitles',\n",
    "    'directors',\n",
    "    'writers'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML.loc[(dbmerge_4['title_ratings_averageRating'].isnull())&(dbmerge_4['tmdb_vote_average'].isnull()), ['title_ratings_tconst', 'title_ratings_averageRating', 'tmdb_vote_average']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML3 = BIG_DF_ML.dropna(subset=['title_ratings_averageRating','tmdb_vote_average'],how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moyenne_ponderee(ligne):\n",
    "\n",
    "    # Si 'title_ratings_averageRating' est NaN, on ne prend que 'tmdb_vote_average', et vice versa\n",
    "\n",
    "    if pd.isna(ligne['title_ratings_averageRating']) and not pd.isna(ligne['tmdb_vote_average']):\n",
    "        return ligne['tmdb_vote_average']  # Si title_ratings_averageRating est vide, prendre tmdb_vote_average\n",
    "\n",
    "    elif pd.isna(ligne['tmdb_vote_average']) and not pd.isna(ligne['title_ratings_averageRating']):\n",
    "        return ligne['title_ratings_averageRating']  # Si tmdb_vote_average est vide, prendre title_ratings_averageRating\n",
    "\n",
    "    elif not pd.isna(ligne['title_ratings_averageRating']) and not pd.isna(ligne['tmdb_vote_average']):\n",
    "        # Si les deux colonnes ont des valeurs, calculer la moyenne pondérée\n",
    "        return (ligne['title_ratings_averageRating'] * ligne['title_ratings_numVotes'] + ligne['tmdb_vote_average'] * ligne['tmdb_vote_count']) / (ligne['title_ratings_numVotes'] + ligne['tmdb_vote_count'])  # Moyenne simple, à ajuster si besoin !\n",
    "    else:\n",
    "        return np.nan  # Si les deux sont NaN, retourner NaN\n",
    "\n",
    "\n",
    "BIG_DF_ML3['notes'] = BIG_DF_ML3.apply(moyenne_ponderee, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML4 = BIG_DF_ML3.drop(['title_ratings_averageRating','tmdb_vote_average','title_ratings_tconst','titleId','tmdb_imdb_id','imdbid', 'primaryName'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startyear(ligne):\n",
    "\n",
    "    # Si 'start year' est 0, il prend la valeur 'tmbd release date'\n",
    "    BIG_DF_ML4.loc[(BIG_DF_ML4['startYear'] == 0)&(BIG_DF_ML4['tmdb_release_date'] != 0), 'startYear'] = BIG_DF_ML4['tmdb_release_date']\n",
    "\n",
    "startyear(BIG_DF_ML4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_date(ligne):\n",
    "\n",
    "    # Si 'tmdn release date' est 0, il prend la valeur 'start year'\n",
    "    BIG_DF_ML4.loc[(BIG_DF_ML4['tmdb_release_date'] == 0)&(BIG_DF_ML4['startYear'] != 0), 'tmdb_release_date'] = BIG_DF_ML4['startYear']\n",
    "\n",
    "release_date(BIG_DF_ML4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_date(ligne):\n",
    "\n",
    "    # Si 'tmdn release date' est 0, il prend la valeur 'start year'\n",
    "    BIG_DF_ML4.loc[(BIG_DF_ML4['tmdb_release_date'].isna())&(BIG_DF_ML4['startYear'] != 0), 'tmdb_release_date'] = BIG_DF_ML4['startYear']\n",
    "\n",
    "release_date(BIG_DF_ML4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtimeMinutes(ligne):\n",
    "\n",
    "    # Si 'runtimeMinutes' est 0, il prend la valeur 'tmdb_runtime'\n",
    "    BIG_DF_ML4.loc[(BIG_DF_ML4['runtimeMinutes'] == 0)&(BIG_DF_ML4['tmdb_runtime'] != 0), 'runtimeMinutes'] = BIG_DF_ML4['tmdb_runtime']\n",
    "\n",
    "runtimeMinutes(BIG_DF_ML4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtimeMinutes(ligne):\n",
    "\n",
    "    # Si 'tmdb_runtime' est 0, il prend la valeur 'runtimeMinutes'\n",
    "    BIG_DF_ML4.loc[(BIG_DF_ML4['tmdb_runtime'] == 0)&(BIG_DF_ML4['runtimeMinutes'] != 0), 'tmdb_runtime'] = BIG_DF_ML4['runtimeMinutes']\n",
    "\n",
    "runtimeMinutes(BIG_DF_ML4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On transforme les valeurs 0 en NaN pour les supprimer ensuite\n",
    "import numpy as np\n",
    "BIG_DF_ML4[BIG_DF_ML4['startYear']==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On transforme les valeurs 0 en NaN pour les supprimer ensuite\n",
    "BIG_DF_ML4[BIG_DF_ML4['runtimeMinutes']==0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5 = BIG_DF_ML4.drop(columns =['tmdb_runtime','tmdb_release_date','tmdb_original_title','tmdb_title','tmdb_vote_count','tmdb_TV Movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5 = BIG_DF_ML5.dropna(subset=['runtimeMinutes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5 = BIG_DF_ML5.dropna(subset=['startYear'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5 = BIG_DF_ML5.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour notre ML, on remplace toutes les valeurs nulles des pays de production par False \n",
    "Remplacer = [\n",
    "    'tmdb_US', 'tmdb_FR', 'tmdb_GB', 'tmdb_DE', 'tmdb_JP', 'tmdb_IN', 'tmdb_IT',\n",
    "    'tmdb_CA', 'tmdb_ES', 'tmdb_MX', 'tmdb_HK', 'tmdb_BR', 'tmdb_SE', 'tmdb_SU',\n",
    "    'tmdb_PH', 'tmdb_KR', 'tmdb_AU', 'tmdb_CN', 'tmdb_AR', 'tmdb_RU', 'tmdb_DK',\n",
    "    'tmdb_NL', 'tmdb_BE', 'tmdb_AT', 'tmdb_TR', 'tmdb_PL', 'tmdb_CH', 'tmdb_XC',\n",
    "    'tmdb_FI', 'tmdb_NO', 'tmdb_IR', 'tmdb_XG', 'tmdb_EG', 'tmdb_NG', 'tmdb_ZA'\n",
    "]\n",
    "BIG_DF_ML5[Remplacer] = BIG_DF_ML5[Remplacer].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5['rating'] = BIG_DF_ML5['rating'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5['title_ratings_numVotes'] = BIG_DF_ML5['title_ratings_numVotes'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5['tmdb_popularity'] = BIG_DF_ML5['tmdb_popularity'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5['nconst'] = BIG_DF_ML5['nconst'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_DF_ML5[['Action', 'Adventure',\n",
    "       'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
    "       'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi',\n",
    "       'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']] = BIG_DF_ML5[['Action', 'Adventure',\n",
    "       'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
    "       'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi',\n",
    "       'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = \"../machine learning/DF_ML.csv.gz\"\n",
    "BIG_DF_ML5.to_csv(export, sep=\",\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = BIG_DF_ML5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de ML avec cible sur les notes > 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les fréquences des valeurs uniques dans la colonne 'nconst'\n",
    "counts = df_ml['nconst'].value_counts()\n",
    "\n",
    "# Filtrer les valeurs qui apparaissent plus de 20 fois\n",
    "valid_nconst = counts[counts > 50].index\n",
    "\n",
    "# Garder uniquement les lignes où 'nconst' est dans valid_nconst\n",
    "filtered_df = df_ml[df_ml['nconst'].isin(valid_nconst)]\n",
    "\n",
    "# Résultat\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml2 = df_ml[df_ml['startYear']>1920]\n",
    "df_ml2['nconst'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut entrainer notre modèle sur tout le dataframe et afficher UNIQUEMENT les k films les plus proches dont les notes sont supérieures à 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation(tconst):\n",
    "\n",
    "     df_ml = pd.read_csv(\"../machine learning/DF_ML.csv.gz\")\n",
    "\n",
    "     index = df_ml.index\n",
    "     df_ml_num = df_ml.select_dtypes('number')\n",
    "     df_ml_cat = df_ml.select_dtypes(['object', 'category', 'string', 'bool'])\n",
    "\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "     SN = MinMaxScaler()\n",
    "     df_ml_num_SN = pd.DataFrame(SN.fit_transform(df_ml_num), columns=df_ml_num.columns, index=index)\n",
    "\n",
    "     df_ml_encoded = pd.concat([df_ml_num_SN, df_ml_cat], axis=1)\n",
    "\n",
    "     #On crée une liste des colonnes à utiliser pour le modèle\n",
    "     caracteristiques = df_ml_encoded.columns.drop(['tconst', 'nconst', 'title', 'tmdb_popularity', 'title_ratings_numVotes','tmdb_US',\n",
    "          'tmdb_FR', 'tmdb_GB', 'tmdb_DE', 'tmdb_JP', 'tmdb_IN', 'tmdb_IT',\n",
    "          'tmdb_CA', 'tmdb_ES', 'tmdb_MX', 'tmdb_HK', 'tmdb_BR', 'tmdb_SE',\n",
    "          'tmdb_SU', 'tmdb_PH', 'tmdb_KR', 'tmdb_AU', 'tmdb_CN', 'tmdb_AR',\n",
    "          'tmdb_RU', 'tmdb_DK', 'tmdb_NL', 'tmdb_BE', 'tmdb_AT', 'tmdb_TR',\n",
    "          'tmdb_PL', 'tmdb_CH', 'tmdb_XC', 'tmdb_FI', 'tmdb_NO', 'tmdb_IR',\n",
    "          'tmdb_XG', 'tmdb_EG', 'tmdb_NG', 'tmdb_ZA'])\n",
    "\n",
    "     #On sépare notre df en deux groupes, en fonction de la note\n",
    "     bons_films = df_ml_encoded[df_ml_encoded['notes'] >= 0.7]\n",
    "     mauvais_films = df_ml_encoded[df_ml_encoded['notes'] < 0.7]\n",
    "\n",
    "     #On crée notre modèle\n",
    "     model = NearestNeighbors(n_neighbors=1000, metric='euclidean')\n",
    "     model.fit(bons_films[caracteristiques])\n",
    "\n",
    "     #On déclare les caractéristiques du film sélectionné par l'utilisateur\n",
    "     caract_film = df_ml_encoded[df_ml_encoded['tconst'] == tconst]\n",
    "     caract_film = caract_film[caracteristiques]\n",
    "     caract_film\n",
    "\n",
    "     distances, indices = model.kneighbors(caract_film)\n",
    "\n",
    "     #On affiche la selection des films en fonction des indices trouvés par le modèle\n",
    "     if caract_film['notes'].values[0] > 0.7:\n",
    "          distances = distances[0][1:11]\n",
    "          indices = indices[0][1:11]\n",
    "          selection = bons_films.iloc[indices]['tconst']\n",
    "     else:\n",
    "          distances = distances[0][0:10]\n",
    "          indices = indices[0][0:10]\n",
    "          selection = bons_films.iloc[indices]['tconst']\n",
    "\n",
    "     return selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = recommandation('tt0099487')\n",
    "pd.DataFrame(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "weights = X_encoded[['Action', 'Adventure',\n",
    "       'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
    "       'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi',\n",
    "       'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']].astype(bool)\n",
    "weights *= 2\n",
    "X_weighted = pd.concat([weights, X_encoded.drop(columns = ['Action', 'Adventure',\n",
    "       'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
    "       'Family', 'Fantasy', 'Game-Show', 'History', 'Horror', 'Music',\n",
    "       'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi',\n",
    "       'Sport', 'Talk-Show', 'Thriller', 'War', 'Western'])], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ML avec TF_IDF (sans les poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = pd.read_csv(\"../gitignore/tmdb_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdb = tmdb[['imdb_id', 'genres', 'overview']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.merge(left=df_ml, right=tmdb, how='left', left_on='tconst', right_on='imdb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation2(tconst):\n",
    "\n",
    "     index = df_test.index\n",
    "     df_test_num = df_test.select_dtypes('number')\n",
    "     df_test_cat = df_test.select_dtypes(['object', 'category', 'string', 'bool'])\n",
    "\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "     SN = MinMaxScaler()\n",
    "     df_test_num_SN = pd.DataFrame(SN.fit_transform(df_test_num), columns=df_test_num.columns, index=index)\n",
    "\n",
    "     df_test_encoded = pd.concat([df_test_num_SN, df_test_cat], axis=1)\n",
    "\n",
    "     #On crée une liste des colonnes à utiliser pour le modèle\n",
    "     caracteristiques = df_test_encoded.columns.drop(['tconst', 'nconst', 'title', 'tmdb_popularity', \n",
    "                                                      'title_ratings_numVotes', 'imdb_id', 'genres', 'overview', 'tmdb_US',\n",
    "          'tmdb_FR', 'tmdb_GB', 'tmdb_DE', 'tmdb_JP', 'tmdb_IN', 'tmdb_IT',\n",
    "          'tmdb_CA', 'tmdb_ES', 'tmdb_MX', 'tmdb_HK', 'tmdb_BR', 'tmdb_SE',\n",
    "          'tmdb_SU', 'tmdb_PH', 'tmdb_KR', 'tmdb_AU', 'tmdb_CN', 'tmdb_AR',\n",
    "          'tmdb_RU', 'tmdb_DK', 'tmdb_NL', 'tmdb_BE', 'tmdb_AT', 'tmdb_TR',\n",
    "          'tmdb_PL', 'tmdb_CH', 'tmdb_XC', 'tmdb_FI', 'tmdb_NO', 'tmdb_IR',\n",
    "          'tmdb_XG', 'tmdb_EG', 'tmdb_NG', 'tmdb_ZA'])\n",
    "\n",
    "     #On sépare notre df en deux groupes, en fonction de la note\n",
    "     bons_films = df_test_encoded[df_test_encoded['notes'] >= 0.7]\n",
    "     mauvais_films = df_test_encoded[df_test_encoded['notes'] < 0.7]\n",
    "\n",
    "     #On crée notre modèle\n",
    "     model = NearestNeighbors(n_neighbors=1000, metric='euclidean')\n",
    "     model.fit(bons_films[caracteristiques])\n",
    "\n",
    "     #On déclare les caractéristiques du film sélectionné par l'utilisateur\n",
    "     caract_film = df_test_encoded[df_test_encoded['tconst'] == tconst]\n",
    "     caract_film = caract_film[caracteristiques]\n",
    "     caract_film\n",
    "\n",
    "     distances, indices = model.kneighbors(caract_film)\n",
    "\n",
    "     #On affiche la selection des films en fonction des indices trouvés par le modèle\n",
    "     colonnes = ['tconst', 'genres', 'overview', 'nconst']  # Liste à étendre si besoin\n",
    "     selection = bons_films.iloc[indices[0]][colonnes]\n",
    "     df_selection = pd.DataFrame(selection).reset_index()\n",
    "\n",
    "     #On crée un nouveau df à partir des données textuelles pour l'étudier avec TF-IDF\n",
    "     df_tfidf = df_selection.copy()\n",
    "     df_tfidf['texte'] = df_selection[colonnes].fillna('').agg(' '.join, axis=1)\n",
    "     df_tfidf = pd.DataFrame(df_tfidf['texte'], columns=['texte'])\n",
    "\n",
    "     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "     #Limite aux 100 mots les plus présents et supprime les mots contenus dans plus de 80% des lignes\n",
    "     vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, max_features=100)\n",
    "     df_tfidf = vectorizer.fit_transform(df_tfidf['texte'])\n",
    "\n",
    "     #On calcule la distance entre chaque vecteur\n",
    "     similarite = cosine_similarity(df_tfidf)\n",
    "\n",
    "     #On crée une liste de tuple avec un index et le score de similarite par rapport au film cible (index 0 car première ligne)\n",
    "     film_index = df_selection.index[0]\n",
    "     similarity_scores = list(enumerate(similarite[film_index]))\n",
    "\n",
    "     #On trie le résultat par rapport aux scores et par ordre decroissant en ignorant la partie index (key permet de cibler uniquement le score)\n",
    "     similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "     similar_movies = [df_selection['tconst'].iloc[i[0]] for i in similarity_scores[1:10]]\n",
    "\n",
    "     return similar_movies\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection2 = recommandation2('tt0099487')\n",
    "selection2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ML avec TF_IDF (avec les poids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation2(tconst):\n",
    "\n",
    "     index = df_test.index\n",
    "     df_test_num = df_test.select_dtypes('number')\n",
    "     df_test_cat = df_test.select_dtypes(['object', 'category', 'string', 'bool'])\n",
    "\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "     SN = MinMaxScaler()\n",
    "     df_test_num_SN = pd.DataFrame(SN.fit_transform(df_test_num), columns=df_test_num.columns, index=index)\n",
    "\n",
    "     df_test_encoded = pd.concat([df_test_num_SN, df_test_cat], axis=1)\n",
    "\n",
    "     #On crée une liste des colonnes à utiliser pour le modèle\n",
    "     caracteristiques = df_test_encoded.columns.drop(['tconst', 'nconst', 'title', 'tmdb_popularity', \n",
    "                                                      'title_ratings_numVotes', 'imdb_id', 'genres', 'overview', 'tmdb_US',\n",
    "          'tmdb_FR', 'tmdb_GB', 'tmdb_DE', 'tmdb_JP', 'tmdb_IN', 'tmdb_IT',\n",
    "          'tmdb_CA', 'tmdb_ES', 'tmdb_MX', 'tmdb_HK', 'tmdb_BR', 'tmdb_SE',\n",
    "          'tmdb_SU', 'tmdb_PH', 'tmdb_KR', 'tmdb_AU', 'tmdb_CN', 'tmdb_AR',\n",
    "          'tmdb_RU', 'tmdb_DK', 'tmdb_NL', 'tmdb_BE', 'tmdb_AT', 'tmdb_TR',\n",
    "          'tmdb_PL', 'tmdb_CH', 'tmdb_XC', 'tmdb_FI', 'tmdb_NO', 'tmdb_IR',\n",
    "          'tmdb_XG', 'tmdb_EG', 'tmdb_NG', 'tmdb_ZA'])\n",
    "\n",
    "     #On sépare notre df en deux groupes, en fonction de la note\n",
    "     bons_films = df_test_encoded[df_test_encoded['notes'] >= 0.7]\n",
    "     mauvais_films = df_test_encoded[df_test_encoded['notes'] < 0.7]\n",
    "\n",
    "     #On crée notre modèle\n",
    "     model = NearestNeighbors(n_neighbors=1000, metric='euclidean')\n",
    "     model.fit(bons_films[caracteristiques])\n",
    "\n",
    "     #On déclare les caractéristiques du film sélectionné par l'utilisateur\n",
    "     caract_film = df_test_encoded[df_test_encoded['tconst'] == tconst]\n",
    "     caract_film = caract_film[caracteristiques]\n",
    "     caract_film\n",
    "\n",
    "     distances, indices = model.kneighbors(caract_film)\n",
    "\n",
    "     #On affiche la selection des films en fonction des indices trouvés par le modèle\n",
    "     colonnes = ['tconst', 'genres', 'overview', 'nconst']  # Liste à étendre si besoin\n",
    "     selection = bons_films.iloc[indices[0]][colonnes]\n",
    "     df_selection = pd.DataFrame(selection).reset_index()\n",
    "\n",
    "     #On crée une colonne pour identifier le réalisateur s'il correspond à celui du film cible\n",
    "     cible_real = df_test.loc[df_test['tconst'] == tconst, 'nconst']\n",
    "     df_selection['real_identique'] = df_selection['nconst'].apply(\n",
    "        lambda x: 1 if x in cible_real else 0\n",
    "     )\n",
    "\n",
    "     #On définit les poids pour chaque colonne textuelle\n",
    "     colonnes_poids = {\n",
    "        'tconst': 1,\n",
    "        'genres': 1,\n",
    "        'overview': 1,\n",
    "        'nconst': 20,\n",
    "        'real_identique': 100\n",
    "     }\n",
    "\n",
    "     #On crée une colonne 'texte' pondérée dynamiquement\n",
    "     df_tfidf = df_selection.copy()\n",
    "     df_tfidf['texte'] = df_selection.apply(\n",
    "     lambda ligne: ' '.join(\n",
    "          (str(ligne[col]) + ' ') * poids for col, poids in colonnes_poids.items()\n",
    "     ),\n",
    "     axis=1\n",
    "     )\n",
    "\n",
    "     #On crée le DataFrame final pour TF-IDF\n",
    "     df_tfidf = pd.DataFrame(df_tfidf['texte'], columns=['texte'])\n",
    "\n",
    "     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "     #Limite aux 100 mots les plus présents et supprime les mots contenus dans plus de 80% des lignes\n",
    "     vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n",
    "     df_tfidf = vectorizer.fit_transform(df_tfidf['texte'])\n",
    "\n",
    "     #On calcule la distance entre chaque vecteur\n",
    "     similarite = cosine_similarity(df_tfidf)\n",
    "\n",
    "     #On crée une liste de tuple avec un index et le score de similarite par rapport au film cible (index 0 car première ligne)\n",
    "     film_index = df_selection.index[0]\n",
    "     similarity_scores = list(enumerate(similarite[film_index]))\n",
    "\n",
    "     #On trie le résultat par rapport aux scores et par ordre decroissant en ignorant la partie index (key permet de cibler uniquement le score)\n",
    "     similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "     similar_movies = [df_selection['tconst'].iloc[i[0]] for i in similarity_scores[1:10]]\n",
    "\n",
    "     return similarity_scores\n",
    "\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Faire un test avec lemmarizing (garder les mots en 'binômes')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation2(tconst):\n",
    "    index = df_test.index\n",
    "    df_test_num = df_test.select_dtypes('number')\n",
    "    df_test_cat = df_test.select_dtypes(['object', 'category', 'string', 'bool'])\n",
    "\n",
    "    # Normalize numerical columns\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    SN = MinMaxScaler()\n",
    "    df_test_num_SN = pd.DataFrame(SN.fit_transform(df_test_num), columns=df_test_num.columns, index=index)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    df_test_cat_encoded = df_test_cat.copy()\n",
    "    \n",
    "    for col in df_test_cat_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_test_cat_encoded[col] = le.fit_transform(df_test_cat_encoded[col].fillna('unknown'))\n",
    "        \n",
    "\n",
    "    # Combine numeric and encoded data\n",
    "    df_test_encoded = pd.concat([df_test_num_SN, df_test_cat_encoded], axis=1)\n",
    "    df_test_encoded['tconst'] = df_test['tconst']  # Add 'tconst' back if it was dropped\n",
    "\n",
    "    # List of features\n",
    "    caracteristiques = df_test_encoded.columns.drop(\n",
    "        ['tconst', 'title', 'tmdb_popularity', 'title_ratings_numVotes', 'imdb_id', 'genres', 'overview', 'tmdb_US',\n",
    "          'tmdb_FR', 'tmdb_GB', 'tmdb_DE', 'tmdb_JP', 'tmdb_IN', 'tmdb_IT',\n",
    "          'tmdb_CA', 'tmdb_ES', 'tmdb_MX', 'tmdb_HK', 'tmdb_BR', 'tmdb_SE',\n",
    "          'tmdb_SU', 'tmdb_PH', 'tmdb_KR', 'tmdb_AU', 'tmdb_CN', 'tmdb_AR',\n",
    "          'tmdb_RU', 'tmdb_DK', 'tmdb_NL', 'tmdb_BE', 'tmdb_AT', 'tmdb_TR',\n",
    "          'tmdb_PL', 'tmdb_CH', 'tmdb_XC', 'tmdb_FI', 'tmdb_NO', 'tmdb_IR',\n",
    "          'tmdb_XG', 'tmdb_EG', 'tmdb_NG', 'tmdb_ZA']\n",
    "    )\n",
    "\n",
    "    # Split into \"good\" and \"bad\" films\n",
    "    bons_films = df_test_encoded[df_test_encoded['notes'] >= 0.7]\n",
    "\n",
    "    # Check if tconst exists\n",
    "    if tconst not in df_test_encoded['tconst'].values:\n",
    "        raise ValueError(f\"tconst {tconst} not found in the dataset.\")\n",
    "\n",
    "    # Select features for the requested film\n",
    "    caract_film = df_test_encoded[df_test_encoded['tconst'] == tconst][caracteristiques]\n",
    "\n",
    "    # NearestNeighbors model\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    model = NearestNeighbors(n_neighbors=1000, metric='euclidean')\n",
    "    model.fit(bons_films[caracteristiques])\n",
    "\n",
    "    distances, indices = model.kneighbors(caract_film)\n",
    "\n",
    "    # Select films found\n",
    "    colonnes = ['tconst', 'genres', 'overview', 'nconst']\n",
    "    selection = bons_films.iloc[indices[0]][colonnes]\n",
    "    df_selection = pd.DataFrame(selection).reset_index()\n",
    "\n",
    "    # Director weighting\n",
    "    target_directors = df_test.loc[df_test['tconst'] == tconst, 'nconst'].unique()\n",
    "    df_selection['director_match'] = df_selection['nconst'].apply(\n",
    "        lambda x: 1 if x in target_directors else 0\n",
    "    )\n",
    "\n",
    "    # Weighted column for text\n",
    "    colonnes_poids = {\n",
    "        'tconst': 1,\n",
    "        'genres': 1,\n",
    "        'overview': 15,\n",
    "        'nconst': 1\n",
    "    }\n",
    "\n",
    "    df_tfidf = df_selection.copy()\n",
    "    df_tfidf['texte'] = df_selection.apply(\n",
    "        lambda ligne: ' '.join(\n",
    "            (str(ligne[col]) + ' ') * poids for col, poids in colonnes_poids.items()\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # TF-IDF and cosine similarity\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8)\n",
    "    df_tfidf_matrix = vectorizer.fit_transform(df_tfidf['texte'])\n",
    "\n",
    "    similarite = cosine_similarity(df_tfidf_matrix)\n",
    "    similarity_scores = list(enumerate(similarite[0]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    similar_movies = [df_selection['tconst'].iloc[i[0]] for i in similarity_scores[1:10]]\n",
    "\n",
    "    return similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection2 = recommandation2('tt1375666')\n",
    "selection2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pour la prochaine fois : utiliser df_selection pour le TFIDF et ne pas avoir les 300K lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "['tt15398776', 'tt0154506', 'tt0482571', 'tt0278504', 'tt0372784', 'tt5013056', 'tt0816692', 'tt6723592', 'tt0468569']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne distance retrouvée après le KNN\n",
    "# Ajouter une colonne distance retrouvée avec le TFIDF après un nouveau KNN uniquement sur le TFIDF \n",
    "# On fait une moyenne pondérée pour trouver le meilleur classement (plus petite en premier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
